{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeplay9800/Sound-Classification-with-Machine-Learning/blob/main/STAT471_FINAL_PROJECT_SUMMARY_NOTEBOOK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMDuaV0VYgE6"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lYL3pAFi7vD"
      },
      "source": [
        "This Jupyter notebook loads the raw amplitude and Mel spectrogram data files as numpy arrays.\n",
        "\n",
        "Download the data files [here](https://console.cloud.google.com/storage/browser/cs181_practical_data).  This notebook assumes that the data files as located in the same directory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh1R_f-C4ySy",
        "outputId": "e0be0dd7-4d51-4588-d670-e6e084246c50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "48z1aQIGi7vF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import dill\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9OftovDw4I18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8720401-3547-4502-e4d7-c3d6a18c0071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pFs7wSh245Ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70311bef-7e9c-4a9a-ed22-293c6730c6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Course Folders/STAT4710 Data Mining/stat471 final project'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Course Folders/STAT4710 Data Mining/stat471 final project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_0_99PSCgh"
      },
      "source": [
        "# 1. Data collection & EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p88IM-gi7vG"
      },
      "source": [
        "### 1.1 Load raw amplitude data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "sRidDXW_i7vG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "03eff79d-4e84-42e9-ebab-d090ec70d6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6c0f67839d3b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/Amp_Data/*_X.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Check that loading in amp data works correctly\n",
        "test_fold = glob.glob('./data/Amp_Data/*_X.npy')\n",
        "print(len(test_fold))\n",
        "x_load = np.load(test_fold[0])\n",
        "X_train = np.empty(0)\n",
        "X_train = np.append(X_train, x_load.flatten())\n",
        "X_train = X_train.reshape(x_load.shape)\n",
        "if np.array_equal(x_load, X_train, equal_nan= True):\n",
        "  print(\"same\")\n",
        "else:\n",
        "  print(\"not same\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPzFoPzh7pyN"
      },
      "outputs": [],
      "source": [
        "#Load Train Data\n",
        "\n",
        "test_fold_X = glob.glob('./data/Amp_Data/*_X.npy')\n",
        "print(len(test_fold_X))\n",
        "\n",
        "X_amp_train = np.empty(0)\n",
        "\n",
        "num_rows_x = 0\n",
        "num_cols_x = 44100\n",
        "for i in range(len(test_fold_X)):\n",
        "  if test_fold_X[i] == './data/Amp_Data/fold10_amp_X.npy':\n",
        "    continue\n",
        "  x_load = np.load(test_fold_X[i])\n",
        "  X_amp_train = np.append(X_amp_train, x_load.flatten())\n",
        "  num_rows_x += x_load.shape[0]\n",
        "\n",
        "print(num_rows_x)\n",
        "X_amp_train = X_amp_train.reshape( num_rows_x, num_cols_x )\n",
        "\n",
        "test_fold_Y = glob.glob('./data/Amp_Data/*_Y.npy')\n",
        "print(len(test_fold_Y))\n",
        "\n",
        "y_amp_train = np.empty(0)\n",
        "\n",
        "num_rows_y = 0\n",
        "for i in range(len(test_fold_Y)):\n",
        "  if test_fold_Y[i] == './data/Amp_Data/fold10_amp_Y.npy':\n",
        "    continue\n",
        "  y_load = np.load(test_fold_Y[i])\n",
        "  y_amp_train = np.append(y_amp_train, y_load.flatten())\n",
        "  num_rows_y += y_load.shape[0]\n",
        "\n",
        "print(y_amp_train.shape)\n",
        "\n",
        "assert num_rows_x == num_rows_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG8LfnMLoV9u"
      },
      "outputs": [],
      "source": [
        "print(test_fold_X)\n",
        "print(test_fold_Y)\n",
        "# fold10 (used for testing) is at idx 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3upz8jVgi7vH"
      },
      "outputs": [],
      "source": [
        "X_amp_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f6v238AFi7vH"
      },
      "outputs": [],
      "source": [
        "# Load Test data\n",
        "test_fold_X = glob.glob('./data/Amp_Data/*_X.npy')\n",
        "print(len(test_fold_X))\n",
        "\n",
        "X_amp_test = np.load(test_fold_X[3])\n",
        "\n",
        "test_fold_Y = glob.glob('./data/Amp_Data/*_Y.npy')\n",
        "print(len(test_fold_Y))\n",
        "\n",
        "y_amp_test = np.load(test_fold_Y[3]).flatten()\n",
        "print(y_amp_test.shape)\n",
        "\n",
        "assert X_amp_test.shape[0] == y_amp_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIxQfmHti7vH"
      },
      "outputs": [],
      "source": [
        "X_amp_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i02GAsdPSXwV"
      },
      "source": [
        "## 1.2 Load spectogram data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXt7qH35bPHn"
      },
      "outputs": [],
      "source": [
        "#Load Spectogram Train Data\n",
        "\n",
        "train_fold_X_spec = glob.glob('./data/Fold_Spec_New/*_X.npy')\n",
        "print(len(train_fold_X_spec))\n",
        "\n",
        "X_spec_train = np.empty(0)\n",
        "\n",
        "num_rows = 128\n",
        "num_cols = 87\n",
        "for i in range(len(train_fold_X_spec)):\n",
        "  if train_fold_X_spec[i] == './data/Fold_Spec_New/fold10_spec_X.npy':\n",
        "    print(\"Found last fold\")\n",
        "    continue\n",
        "  x_load = np.load(train_fold_X_spec[i])\n",
        "  print(x_load.shape)\n",
        "  X_spec_train = np.append(X_spec_train, x_load.flatten())\n",
        "  del x_load # free memory\n",
        "\n",
        "X_spec_train = X_spec_train.reshape( -1 , num_rows *  num_cols)\n",
        "\n",
        "train_fold_Y_Spec = glob.glob('./data/Fold_Spec_New/*_Y.npy')\n",
        "print(len(train_fold_Y_Spec))\n",
        "\n",
        "y_spec_train = np.empty(0)\n",
        "\n",
        "for i in range(len(train_fold_Y_Spec)):\n",
        "  if train_fold_Y_Spec[i] == './data/Fold_Spec_New/fold10_spec_Y.npy':\n",
        "    continue\n",
        "  y_load = np.load(train_fold_Y_Spec[i])\n",
        "  y_spec_train = np.append(y_spec_train, y_load.flatten())\n",
        "  del y_load # free memory\n",
        "\n",
        "print(y_spec_train.shape)\n",
        "\n",
        "assert X_spec_train.shape[0] == y_spec_train.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9US1N8RgYV8a"
      },
      "outputs": [],
      "source": [
        "# Load Spectogram Test data\n",
        "test_fold_X_spec = './data/Fold_Spec_New/fold10_spec_X.npy'\n",
        "\n",
        "X_spec_test = np.load(test_fold_X_spec)\n",
        "X_spec_test = X_spec_test.reshape( -1 , num_rows *  num_cols)\n",
        "\n",
        "test_fold_Y_spec = './data/Fold_Spec_New/fold10_spec_Y.npy'\n",
        "\n",
        "y_spec_test = np.load(test_fold_Y_spec).flatten()\n",
        "print(y_spec_test.shape)\n",
        "\n",
        "assert X_spec_test.shape[0] == y_spec_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79PgVybwYY4F"
      },
      "outputs": [],
      "source": [
        "# plot spectogram data\n",
        "import librosa\n",
        "import librosa.display\n",
        "Load_Data = './data/Fold_Spec_New/fold8_spec_X.npy'\n",
        "mel_spec = np.load(Load_Data)\n",
        "librosa.display.specshow(mel_spec[1].reshape(128, 87))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1-EmwO2Skiv"
      },
      "source": [
        "# 2. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPqIuN_OBc0p"
      },
      "source": [
        "## 2.1 Baseline model (Logistic Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8MhXBvlYykl"
      },
      "source": [
        "### 2.1.1 Trained on amplitude data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCilSyRqBmQi"
      },
      "outputs": [],
      "source": [
        "# Train a Logisc Regression Classifier\n",
        "\n",
        "logistic_classifier = LogisticRegression(random_state = 0, max_iter = 100000)\n",
        "\n",
        "X_amp_train_shortened = X_amp_train[:2000]\n",
        "Y_amp_train_shortened = y_amp_train[:2000]\n",
        "logistic_classifier.fit(X_amp_train_shortened, Y_amp_train_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkuynmf0I2Op"
      },
      "outputs": [],
      "source": [
        "print(logistic_classifier.score(X_amp_train_shortened, Y_amp_train_shortened))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkk5UzDrIh5H"
      },
      "outputs": [],
      "source": [
        "X_amp_test_shortened = X_amp_test[:100]\n",
        "Y_amp_test_shortened = y_amp_test[:100]\n",
        "logistic_classifier.score(X_amp_test_shortened, Y_amp_test_shortened)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yaatdARYsL_"
      },
      "source": [
        "### 2.1.2 Trained on spectogram data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwJBmt8zY6zT"
      },
      "outputs": [],
      "source": [
        "logistic_classifier_spec = LogisticRegression(random_state = 0, max_iter = 1000)\n",
        "\n",
        "# X_amp_train_shortened = X_spec_train[:2000]\n",
        "# Y_amp_train_shortened = y_spec_train[:2000]\n",
        "logistic_classifier_spec.fit(X_spec_train[:2000], y_spec_train[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpXaDQdjZFDI"
      },
      "outputs": [],
      "source": [
        "# getting score on train data\n",
        "print(X_spec_train[2100].shape)\n",
        "logistic_classifier_spec.score(X_spec_train[2000:2060], y_spec_train[2000:2060] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S51ObULQ4J7Z"
      },
      "source": [
        "## 2.2 Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdawG_4L4OHK"
      },
      "source": [
        "We will now run PCA on the feature set of the WAV files (currently 44k dimensions) in order to subsequently run KNN without introducing the effects of the curse of dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB7qq7s0L7Xq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_amp_train_shortened)\n",
        "X_amp_train_scaled_shortened = scaler.transform(X_amp_train_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miO6n1TuX-6t"
      },
      "outputs": [],
      "source": [
        "# Use the same scalar to scale the corresponding testing data\n",
        "X_amp_test_scaled_shortened = scaler.transform(X_amp_test_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkjAPaUVAQrO"
      },
      "outputs": [],
      "source": [
        "import sklearn.decomposition as skdecomp\n",
        "\n",
        "pca = skdecomp.PCA(n_components=2)\n",
        "pca.fit(X_amp_train_scaled_shortened)\n",
        "X_amp_train_pca = pca.transform(X_amp_train_scaled_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LQExCYwHWQ-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.scatter(X_amp_train_pca[:, 0], X_amp_train_pca[:, 1])\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyNPREa8Hzyn"
      },
      "outputs": [],
      "source": [
        "pca = skdecomp.PCA(n_components=3)\n",
        "pca.fit(X_amp_train_scaled_shortened)\n",
        "X_amp_train_pca = pca.transform(X_amp_train_scaled_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecm0UPWxIO5A"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)\n",
        "\n",
        "ax.scatter(X_amp_train_pca[:, 0], X_amp_train_pca[:, 1], X_amp_train_pca[:, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C72t3VcaLhCr"
      },
      "source": [
        "Now, we will proceed with calculating the covariance matrix, and visualize how increasing the number of features explains the variance within the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_spec = StandardScaler()\n",
        "scaler_spec.fit(X_spec_train)\n",
        "X_spec_train = scaler_spec.transform(X_spec_train)\n",
        "X_spec_test = scaler_spec.transform(X_spec_test)"
      ],
      "metadata": {
        "id": "awTApah0AyeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niaK9_AMO_Cf"
      },
      "outputs": [],
      "source": [
        "covar_matrix = skdecomp.PCA(n_components = 3000)\n",
        "covar_matrix.fit(X_spec_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLf0TRQCTbKD"
      },
      "outputs": [],
      "source": [
        "variance = covar_matrix.explained_variance_ratio_\n",
        "\n",
        "var = np.cumsum(np.round(variance, decimals=3)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmZh6bqLgwp"
      },
      "outputs": [],
      "source": [
        "print(var)\n",
        "\n",
        "plt.ylabel('% Variance Explained')\n",
        "plt.xlabel('# of Features')\n",
        "plt.title('PCA Analysis')\n",
        "plt.style.context('seaborn-whitegrid')\n",
        "\n",
        "\n",
        "plt.plot(var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yNqOiZjsVZl"
      },
      "outputs": [],
      "source": [
        "for i in range(10, 100):\n",
        "  print(f\"numFeatures = {i}, % variance explained = {var[i]}\")\n",
        "  # ~55 features is a good choice for numFeatures to include"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSU02hNpeclE"
      },
      "outputs": [],
      "source": [
        "pca_final = skdecomp.PCA(n_components=100)\n",
        "pca_final.fit(X_spec_train)\n",
        "X_spec_train_final_pca = pca_final.transform(X_spec_train)\n",
        "\n",
        "final_variance = pca_final.explained_variance_ratio_\n",
        "\n",
        "final_var = np.cumsum(np.round(final_variance, decimals=3)*100)\n",
        "plt.ylabel('% Variance Explained')\n",
        "plt.xlabel('# of Features')\n",
        "plt.title('PCA Analysis')\n",
        "plt.style.context('seaborn-whitegrid')\n",
        "\n",
        "\n",
        "plt.plot(final_var)\n",
        "# ok now we have reduced dimension data (60-D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naTY0oCSefeC"
      },
      "outputs": [],
      "source": [
        "print(X_spec_train_final_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfr14vG4eooR"
      },
      "source": [
        "### 2.2.1 Logistic classifier on spec PCA features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReEhyQfbeoZA"
      },
      "outputs": [],
      "source": [
        "# now we do some logistic regression on smaller data\n",
        "lg_clf = LogisticRegression(random_state = 0, max_iter = 1000)\n",
        "lg_clf.fit(X_spec_train_final_pca, y_spec_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmLgHGbke1Ku"
      },
      "outputs": [],
      "source": [
        "# pca transform the test data\n",
        "X_spec_test_final_pca = pca_final.transform(X_spec_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDiWxsi-e2fZ"
      },
      "outputs": [],
      "source": [
        "# Test lg classifier on train data\n",
        "lg_clf.score(X_spec_train_final_pca[100:110], y_spec_train[100:110])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLX660eBe5Kn"
      },
      "outputs": [],
      "source": [
        "lg_clf.score(X_spec_test_final_pca, y_spec_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmif7D4lMNlu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "y_pred = lg_clf.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EMTRcfOEWB7"
      },
      "source": [
        "## 2.3 Random Forests classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next classifier model we tested was Random Forests, which takes an ensemble of decision trees, each of which outputs a predicted class label, and takes the majority vote as the random forests classifier prediction.\n"
      ],
      "metadata": {
        "id": "viRHzCuMxQB4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmgRlbphb8u"
      },
      "source": [
        "### 2.3.1 on amp data (reduced dimensionality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w_OHl-WEVgx"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators = 100, random_state = 0)\n",
        "rf_model.fit(X_amp_train_shortened, Y_amp_train_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-gyQCivupWs"
      },
      "outputs": [],
      "source": [
        "train_score = rf_model.score(X_amp_train_shortened, Y_amp_train_shortened)\n",
        "test_score = rf_model.score(X_amp_test_shortened, Y_amp_test_shortened)\n",
        "print(f\"Score on amp train data: {train_score}, Score on amp test data: {test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlfOf_3Mw4DC"
      },
      "source": [
        "### 2.3.2 on amp data (full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96I1hctAw37G"
      },
      "outputs": [],
      "source": [
        "rf_model2 = RandomForestClassifier(n_estimators = 100, random_state = 1)\n",
        "rf_model2.fit(X_amp_train, y_amp_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV8kh3n913-V"
      },
      "outputs": [],
      "source": [
        "train_score = rf_model.score(X_amp_train, y_amp_train)\n",
        "test_score = rf_model.score(X_amp_test, y_amp_test)\n",
        "print(f\"Score on full amp train data: {train_score}\")\n",
        "print(f\"Score on full amp test data: {test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoyxRfHxhHkk"
      },
      "source": [
        "### 2.3.3 on spec data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmtbhyarhHSx"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model_spec = RandomForestClassifier(n_estimators = 100, random_state = 0)\n",
        "rf_model_spec.fit(X_spec_train, y_spec_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4EfJJb9hEO0"
      },
      "outputs": [],
      "source": [
        "spec_train_score = rf_model_spec.score(X_spec_train, y_spec_train)\n",
        "spec_test_score = rf_model_spec.score(X_spec_test, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0p1ak6imfz"
      },
      "source": [
        "### 2.3.4 on spec data (PCA-transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, random forests performed well and had results more generalizable to the test set compared to other classifiers, as it is more robust to overfitting on the training set due to relying on a committee of relatively-uncorrelated individual decision trees."
      ],
      "metadata": {
        "id": "VehI_Js2xaNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzzIQnP8iSur"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model_spec_PCA = RandomForestClassifier(n_estimators = 100, random_state = 0)\n",
        "rf_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sIa-df6idIE"
      },
      "outputs": [],
      "source": [
        "spec_train_score = rf_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = rf_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXuie808Fc91"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = rf_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "cm = confusion_matrix(y_spec_test, y_pred)\n",
        "\n",
        "class_labels = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'enginge_idling', 'gun_shot', 'jackhammer', 'siren','street_music']\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "sns.heatmap(cm, annot = True, fmt = \"d\", xticklabels = class_labels, yticklabels = class_labels)\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel(\"Actual label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the confusion matrix above, we see that Random Forests performed relatively well in categorizing most of the audio sample classes, but struggled\n",
        "for some specific classes such as ’drilling’ and ’siren’, which had several misclassified points. We believe this is likely explained by the fact that audio samples in some of the classes like ’drilling’ are of similar ampli-\n",
        "tude to the other classes, making it hard for the classifier models to distinguish."
      ],
      "metadata": {
        "id": "NhY9de-myzx-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DIyCmpXLlL_"
      },
      "outputs": [],
      "source": [
        "y_pred = rf_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuJRVBz7LmcX"
      },
      "source": [
        "Checking the accuracy, recall, and F1 score above, we see that we observed that the Random Forests classifier performed the best out of the traditional\n",
        "ML models, exhibiting a test accuracy of 67.14% and high scores in the range of 60 − 70% for F1, precision, recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sPqaQ0ByeUx"
      },
      "outputs": [],
      "source": [
        "class_accuracies = np.diag(cm) / np.sum(cm, axis = 1)\n",
        "sorted_class_accuracies = np.sort(class_accuracies)\n",
        "sorted_indices = np.argsort(class_accuracies)\n",
        "sorted_class_labels = np.array(class_labels)[sorted_indices]\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.barh(sorted_class_labels, sorted_class_accuracies)\n",
        "plt.title(\"Class accuracies\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Class label\")\n",
        "\n",
        "for i, v in enumerate(sorted_class_accuracies):\n",
        "    plt.text(v, i, str(round(v, 2)), color='blue', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the individual class accuracies graph above, distinguishable classes like ’gunshot’ tended to have very high class accuracies (91%), whereas the low-distinguishability classes like 'drilling' and 'siren' had lower accuracies (46% and 52%) that drove down the random forests classifier's overall accuracy score."
      ],
      "metadata": {
        "id": "ikF-_L3kzV43"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTBTZzlyvRVt"
      },
      "source": [
        "## 2.4 Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0llGpyOiHws"
      },
      "source": [
        "### 2.4.1 on amp data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRsOLrQwvQ2k"
      },
      "outputs": [],
      "source": [
        "num_classes = len(np.unique(Y_amp_test_shortened))\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjJ-eahA3qY6"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_amp_train_shortened, Y_amp_train_shortened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o38kcsl31La"
      },
      "outputs": [],
      "source": [
        "train_score = nb_model.score(X_amp_train_shortened, Y_amp_train_shortened)\n",
        "test_score = nb_model.score(X_amp_test_shortened, Y_amp_test_shortened)\n",
        "print(f\"Score on shortened amp train data: {train_score}, Score on shortened amp test data: {test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVH9wDN4iCed"
      },
      "source": [
        "### 2.4.2 on spec data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIipAYbthyPi"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model_spec = GaussianNB()\n",
        "nb_model_spec.fit(X_spec_train, y_spec_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUtp7dddh4LA"
      },
      "outputs": [],
      "source": [
        "spec_train_score = nb_model_spec.score(X_spec_train, y_spec_train)\n",
        "spec_test_score = nb_model_spec.score(X_spec_test, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RthnO_Eji5aO"
      },
      "source": [
        "### 2.4.3 on spec data (PCA-transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsxyYaBei8LW"
      },
      "outputs": [],
      "source": [
        "nb_model_spec_PCA = GaussianNB()\n",
        "nb_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWHJM3oBjC99"
      },
      "outputs": [],
      "source": [
        "spec_train_score = nb_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = nb_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSyjYyBhLvFz"
      },
      "outputs": [],
      "source": [
        "y_pred = nb_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOOEvslLj4gW"
      },
      "source": [
        "## 2.5 K-Nearest Neighbor (kNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "was86wWuj6id"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model_spec_PCA = KNeighborsClassifier(n_neighbors = 5)\n",
        "knn_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "spec_train_score = knn_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = knn_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGn4NkxfLzJi"
      },
      "outputs": [],
      "source": [
        "y_pred = knn_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpEaBN7zWejy"
      },
      "source": [
        "Visualization of the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv-HhAsVWd3Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = knn_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "cm = confusion_matrix(y_spec_test, y_pred)\n",
        "\n",
        "class_labels = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'enginge_idling', 'gun_shot', 'jackhammer', 'siren','street_music']\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "sns.heatmap(cm, annot = True, fmt = \"d\", xticklabels = class_labels, yticklabels = class_labels)\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel(\"Actual label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO4Mx1bJXZQt"
      },
      "source": [
        "## 2.6 Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ni7Ex13Xes7"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model_spec_PCA = SVC(kernel = 'rbf', random_state = 0)\n",
        "svm_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "spec_train_score = svm_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = svm_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUu6BbUyL36P"
      },
      "outputs": [],
      "source": [
        "y_pred = svm_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue7qFdvXmS3Z"
      },
      "source": [
        "### 2.6.1 Hyperparameter Tuning (on C and gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUhNqlnTmXx-"
      },
      "source": [
        "**C:** adds a penalty for each misclassified data point. If C is large, SVM tries to minimize the number of misclassified examples due to the high penalty which results in a decision boundary with a smaller margin.\n",
        "\n",
        "**Gamma:** values of gamma indicate a large similarity radius which results in more points being grouped together. For high values of gamma, the points need to be very close to each other in order to be considered in the same group (or class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyIBwVbTlqAi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': ['scale', 'auto'],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "grid.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "print(grid.best_params_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "print(grid.score(X_spec_test_final_pca, y_spec_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV8cfO4-opLM"
      },
      "outputs": [],
      "source": [
        "svm_tuned = SVC(C=10, kernel='rbf', random_state=0)\n",
        "svm_tuned.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "spec_train_score = svm_tuned.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = svm_tuned.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMYK2NaFZeRa"
      },
      "source": [
        "## 2.7 XGBoost classifier (gradient-boosted decision trees)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP6TwKFHZ04b"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model_spec_PCA = XGBClassifier()\n",
        "xgb_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "spec_train_score = xgb_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = xgb_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MevLqQi4L8z_"
      },
      "outputs": [],
      "source": [
        "y_pred = xgb_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_W2PvBtX_sM"
      },
      "source": [
        "# 3. Further Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5mWcN0ea4Ma"
      },
      "source": [
        "## 3.1 Neural networks (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVGxXR59a4A9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_model_spec_PCA = MLPClassifier(hidden_layer_sizes = (1000, 500, 250), max_iter = 2000)\n",
        "mlp_model_spec_PCA.fit(X_spec_train_final_pca, y_spec_train)\n",
        "\n",
        "spec_train_score = mlp_model_spec_PCA.score(X_spec_train_final_pca, y_spec_train)\n",
        "spec_test_score = mlp_model_spec_PCA.score(X_spec_test_final_pca, y_spec_test)\n",
        "print(f\"Score on spec train data: {spec_train_score}\")\n",
        "print(f\"Score on spec test data: {spec_test_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbn42MyzMB9P"
      },
      "outputs": [],
      "source": [
        "y_pred = mlp_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_spec_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "recall = recall_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_spec_test, y_pred, average = \"macro\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ao1_P41joBN"
      },
      "source": [
        "## 3.2 Model 1: Simple neural network (only dense layers with ReLU & softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPIE7_-puI7b"
      },
      "source": [
        "Since the class labels are currently stored as strings, we converted them to one-hot encoding for the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQTcBeejtzW8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_spec_train_one_hot = to_categorical(y_spec_train)\n",
        "y_spec_test_one_hot = to_categorical(y_spec_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyCOWlOatZZC"
      },
      "outputs": [],
      "source": [
        "# X_spec_train_final_pca.shape is (7895, 60)\n",
        "# X_spec_test_final_pca.shape is (837, 60)\n",
        "# y_spec_train.shape is (7895,)\n",
        "# y_spec_test.shape is (837,)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation = 'relu', input_shape = (100,)),\n",
        "    layers.Dense(100, activation = 'relu'),\n",
        "    layers.Dense(100, activation = 'relu'),\n",
        "    layers.Dense(10, activation = 'softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "model.fit(X_spec_train_final_pca, y_spec_train_one_hot, epochs = 50, validation_split=0.1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gkv5gQmz0Lr"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AHW6NSY3aVf"
      },
      "outputs": [],
      "source": [
        "loss = model.history.history['loss']\n",
        "val_loss = model.history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrcOlSgW4Rhg"
      },
      "outputs": [],
      "source": [
        "acc = model.history.history['accuracy']\n",
        "val_acc = model.history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc, label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbAbIb7Wwn1O"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(X_spec_test_final_pca, y_spec_test_one_hot, verbose = 2)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5sjG1AVNCbH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_spec_test_final_pca)\n",
        "y_pred = np.argmax(y_pred, axis = 1)\n",
        "y_actual = np.argmax(y_spec_test_one_hot, axis = 1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_actual, y_pred)}\")\n",
        "print(f\"Recall: {recall_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"Precision: {precision_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"F1 score: {f1_score(y_actual, y_pred, average = 'macro')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDlBLLM47txm"
      },
      "source": [
        "## 3.3 Model 2: Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bVqoziXxAAH"
      },
      "outputs": [],
      "source": [
        "model2 = keras.Sequential([\n",
        "    layers.Conv1D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (100, 1)),\n",
        "    layers.MaxPooling1D(pool_size = 2),\n",
        "    layers.Conv1D(filters = 64, kernel_size = 3, activation = 'relu'),\n",
        "    layers.MaxPooling1D(pool_size = 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation = 'relu'),\n",
        "    layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model2.compile(optimizer = 'adam',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "model2.fit(X_spec_train_final_pca, y_spec_train_one_hot, epochs = 60, validation_split=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__bKR5fx50cv"
      },
      "outputs": [],
      "source": [
        "loss = model2.history.history['loss']\n",
        "val_loss = model2.history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7lzZ68D7Zru"
      },
      "outputs": [],
      "source": [
        "acc = model2.history.history['accuracy']\n",
        "val_acc = model2.history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc, label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOW3IhYK1N57"
      },
      "outputs": [],
      "source": [
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model2.predict(X_spec_test_final_pca)\n",
        "y_pred = np.argmax(y_pred, axis = 1)\n",
        "y_actual = np.argmax(y_spec_test_one_hot, axis = 1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_actual, y_pred)}\")\n",
        "print(f\"Recall: {recall_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"Precision: {precision_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"F1 score: {f1_score(y_actual, y_pred, average = 'macro')}\")"
      ],
      "metadata": {
        "id": "0_4kL_G5Uhxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM7vrqlj2MrQ"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model2.evaluate(X_spec_test_final_pca, y_spec_test_one_hot, verbose = 2)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsy2Cg77wNh"
      },
      "source": [
        "## 3.4 Model 3: CNN with dropout & regularization (to reduce overfitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cFzyCe07w06"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model3 = keras.Sequential([\n",
        "    layers.Conv1D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (100, 1)),\n",
        "    layers.MaxPooling1D(pool_size = 2),\n",
        "    layers.Conv1D(filters = 64, kernel_size = 3, activation = 'relu'),\n",
        "    layers.MaxPooling1D(pool_size = 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation = 'relu', kernel_regularizer = regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model3.compile(optimizer = 'adam',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "hist = model3.fit(X_spec_train_final_pca, y_spec_train_one_hot, epochs = 50, validation_split=0.1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKxoiIlR8P3b"
      },
      "outputs": [],
      "source": [
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuUH1Gq-8RyP"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model3.evaluate(X_spec_test_final_pca, y_spec_test_one_hot, verbose = 2)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model3.predict(X_spec_test_final_pca)\n",
        "y_pred = np.argmax(y_pred, axis = 1)\n",
        "y_actual = np.argmax(y_spec_test_one_hot, axis = 1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_actual, y_pred)}\")\n",
        "print(f\"Recall: {recall_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"Precision: {precision_score(y_actual, y_pred, average = 'macro')}\")\n",
        "print(f\"F1 score: {f1_score(y_actual, y_pred, average = 'macro')}\")"
      ],
      "metadata": {
        "id": "VxmKVx4UlG4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSAIdxSe8mRl"
      },
      "outputs": [],
      "source": [
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq_QoLuU8qE0"
      },
      "outputs": [],
      "source": [
        "acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc, label = 'Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4b Audio Classification Specific CNN"
      ],
      "metadata": {
        "id": "IDpPrdl_rQs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_spec_train_full = X_spec_train.reshape(7895, 128, -1)"
      ],
      "metadata": {
        "id": "Vz0QaHoaIT_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spec_test_full = X_spec_test.reshape(-1, 128, 87)"
      ],
      "metadata": {
        "id": "DEuu5w1Ka1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(X_spec_train_full, open('X_spec_train_full.pkl', 'wb'))\n",
        "pickle.dump(y_spec_train_one_hot, open('y_spec_train_one_hot.pkl', 'wb'))\n",
        "pickle.dump(X_spec_test_full, open('X_spec_test_full.pkl', 'wb'))\n",
        "pickle.dump(y_spec_test_one_hot, open('y_spec_test_one_hot.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "yd_V8rGGPJ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These bigger CNNs are created/trained in a separate Colab notebook."
      ],
      "metadata": {
        "id": "WvzewbXTq8Qm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlqT66RPZQsP"
      },
      "source": [
        "## 3.5 Comparison to State-of-the-art SpeechBrain model (ECAPA-TDNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97KFPb7MZcNc"
      },
      "outputs": [],
      "source": [
        "!pip install speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-gpTxtatDiX"
      },
      "outputs": [],
      "source": [
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "model = EncoderClassifier.from_hparams(source = \"speechbrain/urbansound8k_ecapa\", savedir = \"pretrained_models/urbansound8k_ecapa\")\n",
        "model.modules"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This would involve directly feeding in an array of WAV files. This is too computationally expensive for Colab, but you can try to pass in individual WAV files and call `classify_batch` to classify them!"
      ],
      "metadata": {
        "id": "i3Wg9eBPqajo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4R6MGV5YI9D"
      },
      "source": [
        "# 4. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJi0G5A7AaD6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7kyg4EmHpXU"
      },
      "source": [
        "Visualization of confusion matrix and individual class accuracies for random forests, our best model (excluding neural net models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60RpcNO2F2J3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = rf_model_spec_PCA.predict(X_spec_test_final_pca)\n",
        "cm = confusion_matrix(y_spec_test, y_pred)\n",
        "\n",
        "class_labels = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'enginge_idling', 'gun_shot', 'jackhammer', 'siren','street_music']\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "sns.heatmap(cm, annot = True, fmt = \"d\", xticklabels = class_labels, yticklabels = class_labels)\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel(\"Actual label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x3fUWBAGhiZ"
      },
      "outputs": [],
      "source": [
        "class_accuracies = np.diag(cm) / np.sum(cm, axis = 1)\n",
        "sorted_class_accuracies = np.sort(class_accuracies)\n",
        "sorted_indices = np.argsort(class_accuracies)\n",
        "sorted_class_labels = np.array(class_labels)[sorted_indices]\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.barh(sorted_class_labels, sorted_class_accuracies)\n",
        "plt.title(\"Class accuracies\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Class label\")\n",
        "\n",
        "for i, v in enumerate(sorted_class_accuracies):\n",
        "    plt.text(v, i, str(round(v, 2)), color='blue', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ],
      "metadata": {
        "id": "6zkhtspTA7OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "m8foZuEtCRki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "!jupyter nbconvert --to html /content/drive/MyDrive/\"Course Folders\"/\"STAT4710 Data Mining\"/\"stat471 final project\"/\"STAT471 FINAL PROJECT SUMMARY NOTEBOOK.ipynb\""
      ],
      "metadata": {
        "id": "mT75uMswBDAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZ3nCfVgDPlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZoyxRfHxhHkk",
        "B0llGpyOiHws",
        "kVH9wDN4iCed"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}